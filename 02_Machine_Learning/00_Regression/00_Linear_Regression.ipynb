{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship between the variables and aims to find the best-fit line that minimizes the difference between the predicted and actual values. The goal of linear regression is to make predictions or understand the impact of independent variables on the dependent variable.\n",
    "\n",
    "**Data**\n",
    "\n",
    "- Input: $x$ (measurements, covariates, features, independent variables)\n",
    "- Output: $y$ (response, dependent variable)\n",
    "\n",
    "**Goal**\n",
    "\n",
    "You need to find a regression function $y≈f(x,β)$, where $β$ is the parameter to be estimated from observations.\n",
    "\n",
    "For Simple Linear regression: $y=β_0+β_1x$\n",
    "For Multiple Linear regression: $y=β_0+β_1x_1+β_2x_2+⋯+β_dx_d$, where $d$ is the number of features.\n",
    "\n",
    "A regression method is linear if the prediction fits a linear function of the unknown parameters $β$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Assumptions of Linear Regression\n",
    "\n",
    "- **Linear Regression should be linear in parameters**\n",
    "\n",
    "    The output variable $y$ should be linear in terms of parameter, not necessarily in terms of input variables $x_0$, $x_1$, ..., $x_n$ while it also may be linear depending upon data.\n",
    "\n",
    "    Example:\n",
    "    - $y=β_0+β_1x$ is linear in terms of both inputs and parameters holding true assumption.\n",
    "    - $y=β_0+β_1x^2$ is not linear in terms of inputs but linear in terms of parameters holding true assumption.\n",
    "    - $y=β_0+β_1^2x$ is linear in not terms of inputs but linear in terms of parameters violating the assumption. Hence, it doesn't represent the linear regression model.\n",
    "\n",
    "-  **Error or residuals should have constant variance and no autocorrelation**\n",
    "\n",
    "    In a linear regression model, we aim to predict a dependent variable $y$ using one or more independent variables $x$. The residuals (or errors) are the differences between the observed values and the values predicted by the model. In this regression model, the residuals or erros have same variance(degree of spreadness) for all data points. This is called **homoscedasticity**.\n",
    "    \n",
    "    **Heteroscedasticity** occurs when the residuals have varying variance across the levels of the independent variables. In other words, the spread of the residuals changes depending on the value of $x$.\n",
    "\n",
    "    >**Why it's a problem??**</br>\n",
    "    When heteroscedasticity is present, the standard errors of the coefficients may be biased, leading to incorrect conclusions about the significance of the predictors. This can affect the reliability of hypothesis tests and confidence intervals.\n",
    "\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:901/1*M2187QiB0o0I6r0JfM0JBg.jpeg\">\n",
    "\n",
    "    Similarly, the assumption of no autocorrelation says that the error terms of different observations should not be correlated. In other words, errors or residuals should be independent and identically distributed.\n",
    "\n",
    "    >**Note:** In time-series data, we are likely to suffer from autocorrelation because each data in the next instant depends upon the data at the previous instant. So, error terms are somehow correlated.\n",
    "\n",
    "-  **No multicollinearity**\n",
    "\n",
    "    This assumption is only for input variables in case of multi linear regression. It states that \"any two or more sets of inputs variables should not be perfectly corrlated.\" This means that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy instead of using multiple predictor variables that are heavily correalted to each other.\n",
    "\n",
    "    >**Why is Multicollinearity a Problem??**</br>\n",
    "    >- **Unstable Estimates:** When predictors are highly correlated, the estimates of the regression coefficients become very sensitive to small changes in the model. This can lead to large variances and - instability in the coefficient estimates.\n",
    "    >- **Inflated Standard Errors:** Multicollinearity inflates the standard errors of the coefficients, making it difficult to determine the significance of each predictor.\n",
    "    >- **Interpretation Issues:** It becomes challenging to determine the individual effect of each predictor on the dependent variable because their effects are not distinct.\n",
    "\n",
    "    Example: \n",
    "    - $Volume = Length * Breadth * Height$\n",
    "    - $Area = Length * Breadth$\n",
    "\n",
    "    In such a situation, it is better to drop one of the multiples input variables from the linear regression model due to reasons mentioned above.\n",
    "\n",
    "- **Random Sampling of observations**\n",
    "\n",
    "    The observations for the linear regression should be randomly sampled from any population. Suppose, you're a biologist studying the factors that affect the growth rate of a particular species of plant. You want to build a regression model to understand how different environmental factors (like sunlight, water, and soil type) influence the growth rate. So, you should randomly select plants from various locations within the habitat. This ensures that you capture a diverse range of environmental conditions, leading to a more accurate and generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we consider a single independent variable and a single dependent variable. The relationship between the variables can be represented by the equation:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the dependent variable\n",
    "- `x` is the independent variable\n",
    "- `c` is the y-intercept (the value of `y` when `x` is 0)\n",
    "- `m` is the slope (the change in `y` for a unit change in `x`)\n",
    "\n",
    "In higher dimension this equation becomes:\n",
    "\n",
    "$y = wx + b$\n",
    "\n",
    "The goal is to estimate the values of $w$ and $b$ that best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric and Mathematical Intuition\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:450/1*bACFHSM81VJVr900rLoUog.png\">\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:450/1*7miWhsUBOAYI8VmXuAp32g.png\">\n",
    "\n",
    "Geometrically, linear regression aims to find the line that minimizes the sum of squared distances between the observed data points and the predicted values on the line. The line is chosen such that the vertical distances between the points and the line are minimized.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "$error for x_1 = y_1 - ŷ_1$\n",
    "\n",
    "$error for x_2 = y_2 - ŷ_2$\n",
    "\n",
    "$error for x_3 = y_3 - ŷ_3 = 0 \\quad$ *$∵ [y_3 = ŷ_3]$*\n",
    "\n",
    "Mathematically, the goal is to minimize the sum of squared residuals (also known as the residual sum of squares or RSS) given by:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the observed value of the dependent variable\n",
    "- `ŷ` is the predicted value of the dependent variable based on the regression line\n",
    "\n",
    "The best-fit line is obtained by minimizing RSS, which can be achieved through various optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS) Estimation\n",
    "\n",
    "The most common method to estimate the coefficients (`b` and `w`) in linear regression is the Ordinary Least Squares (OLS) estimation. It aims to minimize the sum of squared residuals by finding the values of `b` and `w` that minimize the following equation:\n",
    "\n",
    "$\\frac{∂RSS}{∂b0} = -2Σ(y - b0 - b1 * x) = 0$\n",
    "\n",
    "$\\frac{∂RSS}{∂b1} = -2Σx(y - b0 - b1 * x) = 0$\n",
    "\n",
    "Solving these equations simultaneously will yield the estimated coefficients b0 and b1:\n",
    "\n",
    "$w = \\frac{Σ(x - x̄)(y - ȳ)}{Σ(x - x̄)^2}$\n",
    "\n",
    "$b = ȳ - w * x̄$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x̄` is the mean of the independent variable `x`\n",
    "- `ȳ` is the mean of the dependent variable `y`\n",
    "\n",
    "These formulas can be computed efficiently, providing the best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "\n",
    "To estimate the coefficients efficiently, optimization methods are used. Two common optimization algorithms are Gradient Descent and Normal Equations.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm that updates the coefficients gradually by descending along the negative gradient of the cost function. The cost function is typically the RSS, and the algorithm seeks to minimize it. The steps of Gradient Descent include:\n",
    "\n",
    "1. Initialize the coefficients randomly or with some initial guess.\n",
    "2. Calculate the predicted values using the current coefficients.\n",
    "3. Calculate the gradient of the cost function with respect to each coefficient.\n",
    "4. Update the coefficients by taking a step proportional to the negative gradient.\n",
    "5. Repeat steps 2-4 until convergence is reached.\n",
    "\n",
    "## Model Evaluation\n",
    "After fitting the linear regression model, it's important to evaluate its performance and assess its quality. Here are some commonly used evaluation metrics:\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "MSE measures the average squared difference between the predicted and actual values. It is computed as:\n",
    "\n",
    "$MSE = \\frac{Σ(y - ŷ)^2}{n}$\n",
    "\n",
    "Where `n` is the number of data points.\n",
    "\n",
    "### R-squared ($R^2$)\n",
    "R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates a perfect fit. It is calculated as:\n",
    "\n",
    "$R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "Where RSS is the residual sum of squares and TSS is the total sum of squares.\n",
    "\n",
    "The RSS (Residual Sum of Squares) represents the sum of squared differences between the observed dependent variable values (y) and the predicted values (ŷ) obtained from the linear regression model. Mathematically, it is calculated as follows:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "On the other hand, the TSS (Total Sum of Squares) represents the total variation in the dependent variable (y) from its mean (ȳ). It measures the sum of squared differences between each observed dependent variable value (y) and the mean of the dependent variable (ȳ). Mathematically, it is calculated as follows:\n",
    "\n",
    "$TSS = Σ(y - ȳ)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "In multiple linear regression, we extend the concept of simple linear regression to include multiple independent variables. The equation for multiple linear regression can be expressed as:\n",
    "\n",
    "$y = b + w_1 * x_1 + w_2 * x_2 + ... + w_n * x_n$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $y$ is the dependent variable\n",
    "- $x_1, x_2, ..., x_n$ are the independent variables\n",
    "- $b$ is the y-intercept\n",
    "- $w_1, w_2, ..., w_n$ are the coefficients for each independent variable\n",
    "\n",
    "The goal is to estimate the values of the coefficients that provide the best-fit hyperplane.\n",
    "\n",
    "### Matrix Formulation\n",
    "\n",
    "To solve multiple linear regression efficiently, we can express it in matrix form. Let's define:\n",
    "\n",
    "- `Y` as a column vector of the dependent variable\n",
    "- `X` as a matrix of independent variables\n",
    "- `B` as a column vector of coefficients\n",
    "- `E` as a column vector of residuals\n",
    "\n",
    "The equation can be rewritten as:\n",
    "\n",
    "$Y = X * B + E$\n",
    "\n",
    "We aim to find the values of B that minimize the sum of squared residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Regularization is a technique used to prevent overfitting and improve the generalization of the linear regression model. Two commonly used regularization methods are Ridge regression and Lasso regression.\n",
    "\n",
    "### Ridge Regression\n",
    "See *Logistic Regression* notebook for more\n",
    "\n",
    "### Lasso Regression\n",
    "See *Logistic Regression* notebook for more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [What Is Linear Regression? Types, Equation, Examples, and Best Practices](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-linear-regression/)\n",
    "- [What is linear regression?](https://www.ibm.com/topics/linear-regression)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
