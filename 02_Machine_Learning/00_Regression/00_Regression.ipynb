{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    " Regression is a widely used statistical technique for modeling the relationship between a dependent variable and one or more independent variables. It assumes a certain kind of relationship between the variables and aims to find the best-fit line that minimizes the difference between the predicted and actual values.\n",
    "\n",
    "**Data**\n",
    "\n",
    "- Input: $x$ (measurements, covariates, features, independent variables)\n",
    "- Output: $y$ (response, dependent variable)\n",
    "\n",
    "**Goal**\n",
    "\n",
    "Find a regression function $y≈f(x,β)$, where $β$ is the parameter to be estimated from observations.\n",
    "\n",
    "For Simple Linear regression: $y=β_0+β_1x$ </br>\n",
    "For Multiple Linear regression: $y=β_0+β_1x_1+β_2x_2+⋯+β_dx_d$, where $d$ is the number of features.\n",
    "\n",
    "A regression method is linear if the prediction fits a linear function of the unknown parameters $β$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Assumptions of Linear Regression\n",
    "\n",
    "- **Linear Regression should be linear in parameters**\n",
    "\n",
    "    The output variable $y$ should be linear in terms of parameter, not necessarily in terms of input variables $x_0$, $x_1$, ..., $x_n$ while it also may be linear depending upon data.\n",
    "\n",
    "    Example:\n",
    "    - $y=β_0+β_1x$ is linear in terms of both inputs and parameters holding true assumption.\n",
    "    - $y=β_0+β_1x^2$ is not linear in terms of inputs but linear in terms of parameters holding true assumption.\n",
    "    - $y=β_0+β_1^2x$ is linear in not terms of inputs but linear in terms of parameters violating the assumption. Hence, it doesn't represent the linear regression model.\n",
    "\n",
    "-  **Error or residuals should have constant variance and no autocorrelation**\n",
    "\n",
    "    In a linear regression model, we aim to predict a dependent variable $y$ using one or more independent variables $x$. The residuals (or errors) are the differences between the observed values and the values predicted by the model. In this regression model, the residuals or erros have same variance(degree of spreadness) for all data points. This is called **homoscedasticity**.\n",
    "    \n",
    "    **Heteroscedasticity** occurs when the residuals have varying variance across the levels of the independent variables. In other words, the spread of the residuals changes depending on the value of $x$.\n",
    "\n",
    "    >**Why it's a problem??**</br>\n",
    "    When heteroscedasticity is present, the standard errors of the coefficients may be biased, leading to incorrect conclusions about the significance of the predictors. This can affect the reliability of hypothesis tests and confidence intervals.\n",
    "\n",
    "    <img src = \"https://miro.medium.com/v2/resize:fit:901/1*M2187QiB0o0I6r0JfM0JBg.jpeg\">\n",
    "\n",
    "    Similarly, the assumption of no autocorrelation says that the error terms of different observations should not be correlated. In other words, errors or residuals should be independent and identically distributed.\n",
    "\n",
    "    >**Note:** In time-series data, we are likely to suffer from autocorrelation because each data in the next instant depends upon the data at the previous instant. So, error terms are somehow correlated.\n",
    "\n",
    "-  **No multicollinearity**\n",
    "\n",
    "    This assumption is only for input variables in case of multi linear regression. It states that \"any two or more sets of inputs variables should not be perfectly corrlated.\" This means that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy instead of using multiple predictor variables that are heavily correalted to each other.\n",
    "\n",
    "    >**Why is Multicollinearity a Problem??**</br>\n",
    "    >- **Unstable Estimates:** When predictors are highly correlated, the estimates of the regression coefficients become very sensitive to small changes in the model. This can lead to large variances and - instability in the coefficient estimates.\n",
    "    >- **Inflated Standard Errors:** Multicollinearity inflates the standard errors of the coefficients, making it difficult to determine the significance of each predictor.\n",
    "    >- **Interpretation Issues:** It becomes challenging to determine the individual effect of each predictor on the dependent variable because their effects are not distinct.\n",
    "\n",
    "    Example: \n",
    "    - $Volume = Length * Breadth * Height$\n",
    "    - $Area = Length * Breadth$\n",
    "\n",
    "    In such a situation, it is better to drop one of the multiples input variables from the linear regression model due to reasons mentioned above.\n",
    "\n",
    "- **Random Sampling of observations**\n",
    "\n",
    "    The observations for the linear regression should be randomly sampled from any population. Suppose, you're a biologist studying the factors that affect the growth rate of a particular species of plant. You want to build a regression model to understand how different environmental factors (like sunlight, water, and soil type) influence the growth rate. So, you should randomly select plants from various locations within the habitat. This ensures that you capture a diverse range of environmental conditions, leading to a more accurate and generalizable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation Techniques\n",
    "\n",
    "There are two techniques to find the parameters(i.e. intercept and regression coefficients) and they are:\n",
    "\n",
    "- **Least Squares Estimation**\n",
    "\n",
    "    We know that observed data or input-output pairs are given, and the coefficients $β'$ s are unknown. We use these input-output pairs to estimate the coefficients. These estimated intercept and regression coefficients help in predicting the final output value using the input values. There remains error or residuals, which can be either positive or negative. To evaluate the estimated linear regression line, we take these errors and sum them up. To avoid the cancellation of negative errors, we square each error before adding them, resulting in the **Sum of Squares of Errors**(SSE) or **Residual Sum of Squares**.\n",
    "\n",
    "    There are mainly three types of techniques that helps in finding the parameters by minimizing the Sum of Squares whcih are:\n",
    "    - Ordinary Least Squares(OLS)\n",
    "\n",
    "    - Weighted Least Squares\n",
    "\n",
    "    - Generalized Least Squares\n",
    "\n",
    "- **Maximum Likelihood Estimation**\n",
    "\n",
    "    Maximum likelihood estimation is a well known probabilistic framework for finding parameters that best describe the observed data. You find the parameters by maximizing the conditional probability of observing the data, $x$ given specific probability distribution and its parameters $β$'s.\n",
    "\n",
    "Among these techniques OLS technique is widely used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
