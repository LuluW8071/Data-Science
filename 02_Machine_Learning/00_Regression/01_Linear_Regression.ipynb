{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "In simple linear regression, we consider a single independent variable and a single dependent variable. The relationship between the variables can be represented by the equation:\n",
    "\n",
    "$y = mx + c$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the dependent variable\n",
    "- `x` is the independent variable\n",
    "- `c` is the y-intercept (the value of `y` when `x` is 0)\n",
    "- `m` is the slope (the change in `y` for a unit change in `x`)\n",
    "\n",
    "In higher dimension this equation becomes:\n",
    "\n",
    "$y = wx + b$\n",
    "\n",
    "The goal is to estimate the values of $w$ and $b$ that best fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric and Mathematical Intuition\n",
    "\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:450/1*bACFHSM81VJVr900rLoUog.png\">\n",
    "<img src = \"https://miro.medium.com/v2/resize:fit:450/1*7miWhsUBOAYI8VmXuAp32g.png\">\n",
    "\n",
    "Geometrically, linear regression aims to find the line that minimizes the sum of squared distances between the observed data points and the predicted values on the line. The line is chosen such that the vertical distances between the points and the line are minimized.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "$error for x_1 = y_1 - ŷ_1$\n",
    "\n",
    "$error for x_2 = y_2 - ŷ_2$\n",
    "\n",
    "$error for x_3 = y_3 - ŷ_3 = 0 \\quad$ *$∵ [y_3 = ŷ_3]$*\n",
    "\n",
    "Mathematically, the goal is to minimize the sum of squared residuals (also known as the residual sum of squares or RSS) given by:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "Where:\n",
    "\n",
    "- `y` is the observed value of the dependent variable\n",
    "- `ŷ` is the predicted value of the dependent variable based on the regression line\n",
    "\n",
    "The best-fit line is obtained by minimizing RSS, which can be achieved through various optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Estimation for Simple Linear Regression\n",
    "\n",
    "<img src = \"../../assets/ordinary_least_square.png\">\n",
    "\n",
    "The most common method to estimate the coefficients (`b` and `w`) in linear regression is the Ordinary Least Squares (OLS) estimation. It aims to minimize the sum of squared residuals by finding the values of `b` and `w` that minimize the following equation:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$\\frac{∂SSE}{∂b_0} = -2Σ(y - b_0 - b_1 * x) = 0$\n",
    "\n",
    "$\\frac{∂SSE}{∂b_1} = -2Σx(y - b_0 - b_1 * x) = 0$\n",
    "</div>\n",
    "\n",
    "Solving these equations simultaneously will yield the estimated coefficients $b_0$ and $b_1$:\n",
    "<div align=\"center\">\n",
    "\n",
    "$w = \\frac{Σ(x - x̄)(y - ȳ)}{Σ(x - x̄)^2}$\n",
    "\n",
    "$b = ȳ - w * x̄$\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "\n",
    "- `x̄` is the mean of the independent variable `x`\n",
    "- `ȳ` is the mean of the dependent variable `y`\n",
    "\n",
    "These formulas can be computed efficiently, providing the best-fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares (OLS) Estimation for Multiple Linear Regression\n",
    "\n",
    "Multiple linear regression generalizes from simple linear regression by allowing more than one input variable $x_1$,$x_2$,..$x_d$. The ultimate goal is to find a relationship between input variables and the output variables.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "$\\hat{y} = β_0 + β_1x_1 + β_2x_2 + ... +  β_dx_d$\n",
    "\n",
    "Where,\n",
    "-  $β_1$ through $β_d$ are the regression coefficients for inpendent variables $x_1$ through $x_d$.\n",
    "\n",
    "Similar to linear regression, the regression model to get actual output variable is:\n",
    "\n",
    "$y = β_0 + β_1x_1 + β_2x_2 + ... +  β_dx_d + ϵ$\n",
    "\n",
    "Where,\n",
    "- ϵ is random error or residual that represents the difference between actual output and predicted outputs.\n",
    "\n",
    "In matrix form,\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\ \n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_n\n",
    "\\end{bmatrix} =   \\begin{bmatrix}\n",
    "  1 & x_{11} & \\cdots & x_{1d} \\\\\n",
    "  1 & x_{21} & \\cdots & x_{2d} \\\\\n",
    "  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "  1 & x_{n1} & \\cdots & x_{nd}\n",
    " \\end{bmatrix}\\begin{bmatrix}\n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_d\n",
    "\\end{bmatrix}+ \\begin{bmatrix}\n",
    "\\epsilon_1 \\\\ \n",
    "\\epsilon_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\epsilon_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "</div>\n",
    "\n",
    "We can write as,</br>\n",
    "<div align=\"center\">\n",
    "\n",
    "$y = Xβ + ϵ$\n",
    "</div>\n",
    "\n",
    "Also,</br>\n",
    "<div align=\"center\">\n",
    "\n",
    "$\\text{SSE} =\\sum_{i=1}^{n}\\epsilon_i^2 =  \\epsilon^T\\epsilon = (y−Xβ)^T(y−Xβ)$\n",
    "</div>\n",
    "\n",
    "Solving for β through differentiating SSE both sides w.r.t. β and setting up by 0. We get estimated parameter from the normal equation as,\n",
    "<div align=\"center\">\n",
    "\n",
    "$\\hat{β}=(X^TX)^{−1} X^Ty$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Methods\n",
    "\n",
    "To estimate the coefficients efficiently, optimization methods are used. Two common optimization algorithms are Gradient Descent and Normal Equations.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm that updates the coefficients gradually by descending along the negative gradient of the cost function. The cost function is typically the RSS, and the algorithm seeks to minimize it. The steps of Gradient Descent include:\n",
    "\n",
    "1. Initialize the coefficients randomly or with some initial guess.\n",
    "2. Calculate the predicted values using the current coefficients.\n",
    "3. Calculate the gradient of the cost function with respect to each coefficient.\n",
    "4. Update the coefficients by taking a step proportional to the negative gradient.\n",
    "5. Repeat steps 2-4 until convergence is reached.\n",
    "\n",
    "## Model Evaluation\n",
    "After fitting the linear regression model, it's important to evaluate its performance and assess its quality. Here are some commonly used evaluation metrics:\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "MSE measures the average squared difference between the predicted and actual values. It is computed as:\n",
    "\n",
    "$MSE = \\frac{Σ(y - ŷ)^2}{n}$\n",
    "\n",
    "Where `n` is the number of data points.\n",
    "\n",
    "### R-squared ($R^2$)\n",
    "R-squared represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates a perfect fit. It is calculated as:\n",
    "\n",
    "$R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "Where RSS is the residual sum of squares and TSS is the total sum of squares.\n",
    "\n",
    "The RSS (Residual Sum of Squares) represents the sum of squared differences between the observed dependent variable values (y) and the predicted values (ŷ) obtained from the linear regression model. Mathematically, it is calculated as follows:\n",
    "\n",
    "$RSS = Σ(y - ŷ)^2$\n",
    "\n",
    "On the other hand, the TSS (Total Sum of Squares) represents the total variation in the dependent variable (y) from its mean (ȳ). It measures the sum of squared differences between each observed dependent variable value (y) and the mean of the dependent variable (ȳ). Mathematically, it is calculated as follows:\n",
    "\n",
    "$TSS = Σ(y - ȳ)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [What Is Linear Regression? Types, Equation, Examples, and Best Practices](https://www.spiceworks.com/tech/artificial-intelligence/articles/what-is-linear-regression/)\n",
    "- [What is linear regression?](https://www.ibm.com/topics/linear-regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
