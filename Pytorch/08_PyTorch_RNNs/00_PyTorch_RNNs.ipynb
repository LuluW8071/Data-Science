{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch RNNs\n",
        "\n",
        "RNNs are a class of artificial neural networks that allow previous outputs to be used as inputs while having hidden states. It consists of connections between nodes that can create cycles, allowing information to persist. This structure enables RNNs to exhibit temporal dynamic behavious, making them well suited for tasks that involve sequential data, such as **time series forecasting**, **natural language processing** and **speech recognition**.\n",
        "\n",
        "<img src=\"https://easyai.tech/wp-content/uploads/2022/08/f0116-2019-07-02-rnn-1.gif\">\n",
        "\n",
        "RNNs have an **internal state** (memory) that is updated each time step based on the current input and the previous state, enabling them to capture and remember dependencies over time.\n",
        "\n",
        ">However, this vanilla RNNs often suffer from issues like **vanishing** & **exploding gradients**, making it dfficult for them to learn long term dependencies.\n",
        "\n",
        "## How RNN differ from Feedforward Neural Network?\n",
        "\n",
        "Unlike traditional feedforward neural networks, RNNs have recurrent connections, allowing information to persist. Each neuron in the network is not only connected to the next layer but also to itself from the previous time step. This cyclic connection enables the network to capture temporal dependencies in the data.\n",
        "\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/assets_-LvBP1svpACTB1R1x_U4_-LwEQnQw8wHRB6_2zYtG_-LwEZT8zd07mLDuaQZwy_image-1.png\">\n",
        "\n",
        "| **Aspect**                | **Feedforward Neural Networks (FNNs)**                                   | **Recurrent Neural Networks (RNNs)**                                                   |\n",
        "|---------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------------------|\n",
        "| **Direction of Information Flow** | Information flows in one direction, from input nodes to output nodes, in a single pass | Information flows in both directions, allowing feedback loops to capture sequential dependencies |\n",
        "| **Feedback Connections**  | No feedback connections                                                  | Feedback connections allow the network to maintain an internal state, enabling it to learn from previous outputs |\n",
        "| **State-Based Representation** | No internal state; only input-output relationships are modeled       | Internal state (or memory) is used to store information, allowing the network to capture long-term dependencies |\n",
        "| **Sequential Processing** | Processes data in parallel, without considering sequential relationships | Designed to process sequential data, where the output of the previous time step is used as input for the next time step |\n",
        "| **Training**              | Trains the network to predict the output for a single input              | Trains the network to predict the output for a sequence of inputs, considering the internal state and feedback connections |\n",
        "| **Applications**          | Suitable for applications like image and speech recognition, where the input can be partitioned into separate, unrelated parts | Well-suited for applications that involve sequential data, such as language translation, speech recognition, and time series forecasting |\n",
        "| **Complexity**            | Generally simpler to implement and train, with fewer parameters to learn | More complex to implement and train, with many more parameters to learn and the need to manage the internal state |\n",
        "| **Error Propagation**     | Error is propagated only through the forward pass, without considering the internal state | Error is propagated backwards through time, allowing the network to adjust the internal state and learn from previous mistakes |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_QhJKlROX2RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unfolding in Recurrent Neural Networks (RNNs)\n",
        "\n",
        "**Unfolding** is a way to visualize and understand how Recurrent Neural Networks (RNNs) process sequential data over time. When we unfold an RNN, we break down its operations into individual time steps, revealing how information flows through the network at each step.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*SKGAqkVVzT6co-sZ29ze-g.png\">\n",
        "\n",
        "#### Key Concepts:\n",
        "\n",
        "1. **Sequential Processing**:\n",
        "   - RNNs process input data one time step at a time. At each time step t, the RNN takes the current input x<sub>t</sub> and the hidden state from the previous time step h<sub>t-1</sub>.\n",
        "\n",
        "2. **Hidden State**:\n",
        "   - The hidden state h<sub>t</sub> serves as the memory of the network, capturing information from previous inputs. It is updated at each time step based on the current input and the previous hidden state.\n",
        "   - h<sub>t</sub> = f(W<sub>h</sub> ⋅ h<sub>t-1</sub> + W<sub>x</sub> ⋅ x<sub>t</sub> + b)\n",
        "   - Here, W<sub>h</sub> and W<sub>x</sub> are weight matrices, b is a bias vector, and f is an activation function (e.g., tanh or ReLU).\n",
        "\n",
        "3. **Unfolding Process**:\n",
        "   - When we unfold an RNN, we create a sequence of repeated copies of the network, each corresponding to a different time step. This allows us to see how the network evolves over time.\n",
        "   - For a sequence of inputs x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>T</sub>, the RNN is unfolded into T time steps, with each step having its own hidden state h<sub>t</sub> and output y<sub>t</sub>.\n",
        "\n",
        "### [Detailed Explanation on RNNs](https://www.deeplearningbook.org/contents/rnn.html)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4xyvsEyad4bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Import Libraries"
      ],
      "metadata": {
        "id": "aBWOWlSNkz6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tacU1mfZV-Ya"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download and Prepare Datasets"
      ],
      "metadata": {
        "id": "9LdJ9mHbk3VB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "!gdown https://drive.google.com/uc?id=1eL4Hxm6NMcR6dCwuGDSPmVi-4BkUGKDz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9SmYm0Ej8IE",
        "outputId": "73b03919-31a2-474a-c953-ac190d842bb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eL4Hxm6NMcR6dCwuGDSPmVi-4BkUGKDz\n",
            "To: /content/language_data.zip\n",
            "\r  0% 0.00/2.88M [00:00<?, ?B/s]\r100% 2.88M/2.88M [00:00<00:00, 115MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Extract zip data\n",
        "with zipfile.ZipFile('language_data.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')"
      ],
      "metadata": {
        "id": "wxBhH6X5m2TR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import unicodedata, string, glob\n",
        "import random\n",
        "\n",
        "# alphabet small + capital letters + \".,;'\"\n",
        "ALL_LETTERS = string.ascii_letters + \".,;'\"\n",
        "\n",
        "# Turn a unicode string to plain ASCII\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "      and c in ALL_LETTERS\n",
        "  )\n",
        "\n",
        "def load_data(path):\n",
        "  # Build the category_lines in dictionary, a list of names per lanaguge\n",
        "  category_lines = {}\n",
        "  all_categories = []\n",
        "\n",
        "  def find_files(path):\n",
        "    return glob.glob(path)\n",
        "\n",
        "  # print(find_files(path))\n",
        "\n",
        "  # Read a file and split into lines\n",
        "  def read_lines(filename):\n",
        "    lines = io.open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode_to_ascii(line) for line in lines]\n",
        "\n",
        "  for filename in find_files(path):\n",
        "    category = os.path.splitext(os.path.basename(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = read_lines(filename)\n",
        "    category_lines[category] = lines\n",
        "\n",
        "  return category_lines, all_categories"
      ],
      "metadata": {
        "id": "4WTdXHKDj8AQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot-Encoding\n",
        "\n",
        "One-hot encoding is a technique used in machine learning to represent categorical data numerically. Each category is represented by a binary vector where only one bit is hot (1) while all others are cold (0), indicating the presence or absence of that category.\n",
        "\n",
        "<img src = \"https://miro.medium.com/v2/resize:fit:2000/1*e70626LPGPlViXMDemmqRQ.png\">\n",
        "\n",
        "Simply, it is a one-hot vector which is filled with 0s except for a 1 at index of a current letter.\n"
      ],
      "metadata": {
        "id": "V7n4MidyrO1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_LETTERS = len(ALL_LETTERS)\n",
        "\n",
        "# Find letter index from all_letters\n",
        "def letter_to_index(letter):\n",
        "  return ALL_LETTERS.find(letter)\n",
        "\n",
        "# Turn a letter into <1 x n_letters> Tensor\n",
        "def letter_to_tensor(letter):\n",
        "  tensor = torch.zeros(1, N_LETTERS)\n",
        "  tensor[0][letter_to_index(letter)] = 1\n",
        "  return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>\n",
        "# or an array of one-hot letter vectors [One-Hot_Encoding]\n",
        "def line_to_tensor(line):\n",
        "  tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
        "  for i, letter in enumerate(line):\n",
        "    tensor[i][0][letter_to_index(letter)] = 1\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "ocpDUgPAqsgy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display results of above functions\n",
        "text = ['Dépatouiller',                             # French\n",
        "        'Dampfschifffahrtsgesellschaftskapitän',    # German\n",
        "        'Kōfuku']                                   # Japanese\n",
        "\n",
        "print(ALL_LETTERS, '\\n')\n",
        "\n",
        "for sample in text:\n",
        "  print(unicode_to_ascii(sample))    # Removes dialect symbols in words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW22egjCt12F",
        "outputId": "35cd8136-e043-4bb4-9d3c-a88ba4519497"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,;' \n",
            "\n",
            "Depatouiller\n",
            "Dampfschifffahrtsgesellschaftskapitan\n",
            "Kofuku\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing file contents\n",
        "path = 'data/names/*.txt'\n",
        "\n",
        "category_lines, all_categories = load_data(path)\n",
        "print(category_lines['Japanese'][:5])\n",
        "\n",
        "# Tensor Mapping after One-Hot-Encoding\n",
        "print(letter_to_index('A'))\n",
        "print(letter_to_tensor('A'))\n",
        "\n",
        "print(line_to_tensor('PyTorch'))\n",
        "print(line_to_tensor('PyTorch').shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERM8wIs_x4f4",
        "outputId": "0d6a6fce-272e-46ff-9ba6-06a4e4d5be70"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Abe', 'Abukara', 'Adachi', 'Aida', 'Aihara']\n",
            "26\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0.]])\n",
            "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "          0., 0., 0., 0., 0.]]])\n",
            "torch.Size([7, 1, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Building RNN"
      ],
      "metadata": {
        "id": "ELdmKlGj-xf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_training_example(category_lines, all_categories):\n",
        "  def random_choice(a):\n",
        "    random_idx = random.randint(0, len(a)-1)\n",
        "    return a[random_idx]\n",
        "\n",
        "  category = random_choice(all_categories)\n",
        "  line = random_choice(category_lines[category])\n",
        "  category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
        "  line_tensor = line_to_tensor(line)\n",
        "\n",
        "  return category, line, category_tensor, line_tensor"
      ],
      "metadata": {
        "id": "GW0fX4apy-Lh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hUeDRiRB1oa_"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}