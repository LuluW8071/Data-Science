{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LuluW8071/Data-Science/blob/main/Pytorch/08_PyTorch_RNNs/01_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "## Embeddings in Natural Language Processing (NLP)\n",
        "\n",
        "In the context of natural language processing (NLP), \"embeddings\" refer to dense vector representations of words (or sometimes phrases and sentences) in a continuous vector space. These vector representations are learned through unsupervised machine learning techniques like Word2Vec, GloVe, or FastText, where words with similar meanings or appearing in similar contexts are mapped to vectors that are close together in the vector space.\n",
        "\n",
        "![Embeddings](https://miro.medium.com/max/600/1*UCKRYEj85S3eH1uv1vFfCw.gif)\n",
        "\n",
        "## Limitations of Traditional Word Representations\n",
        "\n",
        "### One-Hot Encoding\n",
        "Traditionally, words have been represented using one-hot encoding, where each word is represented as a sparse binary vector. In this representation, there is a 1 in the position corresponding to the word's index in the vocabulary and 0s everywhere else. However, one-hot encoded vectors have several limitations:\n",
        "\n",
        "![One hot encoding](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
        "\n",
        "- **High Dimensionality:** One-hot encoded vectors are very high-dimensional, with the dimensionality equal to the size of the vocabulary. This leads to increased computational complexity and storage requirements.\n",
        "- **Lack of Semantic Information:** One-hot vectors do not capture any semantic relationships between words. Each word is treated as an isolated entity with no notion of similarity or relatedness to other words.\n",
        "\n",
        "## Advantages of Word Embeddings\n",
        "\n",
        "Embeddings address the limitations of one-hot encoding and offer several advantages in NLP:\n",
        "\n",
        "### 1. Low-Dimensional Dense Representations\n",
        "Word embeddings are low-dimensional dense vectors, typically ranging from 50 to 300 dimensions. This makes them computationally efficient and memory-friendly compared to one-hot vectors.\n",
        "\n",
        "### 2. Semantic Relationships\n",
        "Embeddings capture semantic relationships between words. Words with similar meanings or appearing in similar contexts will have similar vector representations, enabling models to understand the meaning and context of words.\n",
        "\n",
        "### 3. Generalization\n",
        "Word embeddings allow NLP models to generalize better across different tasks and datasets. Pre-trained word embeddings can be used as features for various downstream tasks, even if the training data for the downstream task is limited.\n",
        "\n",
        "### 4. Out-of-Vocabulary (OOV) Words\n",
        "Word embeddings provide representations for words not seen during training (OOV words) by generalizing from the context of other words.\n",
        "\n",
        "### 5. Efficiency\n",
        "Once trained, word embeddings can be efficiently stored and reused, which is especially important for large-scale NLP applications.\n",
        "\n",
        "### 6. Capturing Analogies\n",
        "Word embeddings can capture analogical relationships like \"king\" - \"man\" + \"woman\" â‰ˆ \"queen,\" allowing models to perform analogy-based reasoning.\n",
        "\n",
        "In summary, embeddings are a powerful tool in NLP, offering a more efficient, semantically rich, and generalizable way to represent words compared to traditional methods like one-hot encoding."
      ],
      "metadata": {
        "id": "x6IdOlDbm0Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words in a continuous vector space. These embeddings capture semantic relationships between words, allowing machines to understand and work with words in a more meaningful way. Word2Vec was introduced by researchers at Google in 2013 and has since become one of the foundational techniques in natural language processing (NLP) and related fields.\n",
        "\n",
        "The basic idea behind Word2Vec is to represent each word in a high-dimensional vector space, where words with similar meanings or contexts are located close to each other. This is based on the distributional hypothesis, which posits that words appearing in similar contexts tend to have similar meanings. For example, in the sentences \"I love cats\" and \"I adore felines,\" the words \"love\" and \"adore\" are likely to be used in similar contexts and have similar semantic meanings.\n",
        "\n",
        "Word2Vec can be trained using two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. Let's explore each of these in detail:\n",
        "\n",
        "<img src=\"https://github.com/LuluW8071/Data-Science/blob/main/assets/cbow_skipgram.png?raw=1\">\n",
        "\n",
        "### 1. Continuous Bag of Words (CBOW)\n",
        "\n",
        "CBOW aims to predict a target word based on its surrounding context words. Given a sequence of words in a sentence, CBOW tries to predict the middle word based on the surrounding context words. The context window size determines how many words before and after the target word are considered as the context.\n",
        "\n",
        "#### Example:\n",
        "Consider the sentence: \"The cat sat on the mat.\" If we set the context window size to 2 and assume \"sat\" is the target word, CBOW will use the context words \"The,\" \"cat,\" \"on,\" and \"the\" to predict the word \"sat.\"\n",
        "\n",
        "#### Architecture:\n",
        "The architecture involves the following steps:\n",
        "- Convert the context words to their corresponding word embeddings.\n",
        "- Average these embeddings to create a context vector.\n",
        "- Use this context vector as input to a neural network to predict the target word.\n",
        "\n",
        "## Implementing Word2Vec in Python\n",
        "\n",
        "Python provides a package named `gensim` to make implementing Word2Vec straightforward. Here's how you can get started with it:\n",
        "\n",
        "### Installation\n",
        "First, install the `gensim` package if you haven't already:\n",
        "\n",
        "```bash\n",
        "pip install gensim\n",
        "```\n",
        "\n",
        "### Explanation of Parameters:\n",
        "- `sentences`: The input data, which is a list of tokenized sentences.\n",
        "- `vector_size`: The dimensionality of the word vectors.\n",
        "- `window`: The maximum distance between the current and predicted word within a sentence.\n",
        "- `min_count`: Ignores all words with total frequency lower than this.\n",
        "- `workers`: The number of worker threads to train the model.\n",
        "\n",
        "Word2Vec with `gensim` is a powerful tool that simplifies the creation of word embeddings, making it easier to integrate semantic understanding into your NLP applications."
      ],
      "metadata": {
        "id": "8oF6Q2AConJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUYW6vf2mpCe",
        "outputId": "979c1018-1f39-49ae-9e21-004b0627bf3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the NLTK tokenizer data if not already done\n",
        "nltk.download('punkt')\n",
        "\n",
        "# List of sentences for training (unsplit)\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Natural language processing is exciting\",\n",
        "    \"Word2Vec creates word embeddings\",\n",
        "    \"Gensim is a useful library\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"Embeddings capture semantic relationships\",\n",
        "    \"Python is a popular programming language\",\n",
        "    \"Artificial intelligence and machine learning are related fields\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK0BXnrWotml",
        "outputId": "f75dfedc-f574-496b-b486-bc02e7c9d924"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model using the tokenized sentences\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "R2bVHh_MpbVC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find similar words using the trained model\n",
        "def find_similar_words(model, word, top_n=5):\n",
        "    vector = model.wv[word]\n",
        "    similar_words = model.wv.most_similar(positive=[word], topn=top_n)\n",
        "    return vector, similar_words"
      ],
      "metadata": {
        "id": "W_58w5tLqElP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector and most similar words for a specific word\n",
        "target_word='machine'\n",
        "vector, similar_words = find_similar_words(model, target_word)\n",
        "\n",
        "print(\"Vector for 'machine':\", vector)\n",
        "print(\"Words most similar to 'machine':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-wPDalqqbpv",
        "outputId": "30e2929a-bb49-4568-bee3-db2c89996a43"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine': [ 9.0809692e-05  3.0832055e-03 -6.8151313e-03 -1.3689728e-03\n",
            "  7.6685268e-03  7.3423618e-03 -3.6741595e-03  2.6473312e-03\n",
            " -8.3173197e-03  6.2057734e-03 -4.6351124e-03 -3.1670930e-03\n",
            "  9.3112951e-03  8.7239651e-04  7.4903150e-03 -6.0771578e-03\n",
            "  5.1645460e-03  9.9195987e-03 -8.4572462e-03 -5.1375022e-03\n",
            " -7.0665088e-03 -4.8636729e-03 -3.7799729e-03 -8.5374974e-03\n",
            "  7.9519451e-03 -4.8466586e-03  8.4186336e-03  5.2713170e-03\n",
            " -6.5517426e-03  3.9549218e-03  5.4736012e-03 -7.4305790e-03\n",
            " -7.4054408e-03 -2.4740247e-03 -8.6299535e-03 -1.5781232e-03\n",
            " -3.9694359e-04  3.3004046e-03  1.4376161e-03 -8.7451038e-04\n",
            " -5.5918437e-03  1.7300018e-03 -8.9923030e-04  6.7969901e-03\n",
            "  3.9745839e-03  4.5312811e-03  1.4351372e-03 -2.7016769e-03\n",
            " -4.3661897e-03 -1.0324767e-03  1.4385569e-03 -2.6458562e-03\n",
            " -7.0720618e-03 -7.8036557e-03 -9.1262041e-03 -5.9363050e-03\n",
            " -1.8445110e-03 -4.3226061e-03 -6.4571970e-03 -3.7157002e-03\n",
            "  4.2899637e-03 -3.7400872e-03  8.3837649e-03  1.5315602e-03\n",
            " -7.2425385e-03  9.4318893e-03  7.6317666e-03  5.4961131e-03\n",
            " -6.8513905e-03  5.8209687e-03  4.0058908e-03  5.1868521e-03\n",
            "  4.2576790e-03  1.9383364e-03 -3.1670255e-03  8.3557712e-03\n",
            "  9.6082436e-03  3.7972576e-03 -2.8360703e-03  3.4182870e-06\n",
            "  1.2130676e-03 -8.4575703e-03 -8.2244556e-03 -2.2725610e-04\n",
            "  1.2358051e-03 -5.7472461e-03 -4.7265878e-03 -7.3482059e-03\n",
            "  8.3350940e-03  1.2313284e-04 -4.5144265e-03  5.7042022e-03\n",
            "  9.1793118e-03 -4.1021546e-03  7.9718847e-03  5.3714202e-03\n",
            "  5.8786725e-03  5.1741279e-04  8.2144625e-03 -7.0180092e-03]\n",
            "Words most similar to 'machine': [('Natural', 0.1991470903158188), ('Word2Vec', 0.17297668755054474), ('intelligence', 0.17133425176143646), ('language', 0.17026880383491516), ('Deep', 0.152846097946167)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Skip-gram\n",
        "\n",
        "Skip-gram, on the other hand, works in the opposite manner to CBOW. It aims to predict the context words given a target word. Skip-gram is particularly useful for smaller datasets and when you want to capture more information about infrequent words.\n",
        "\n",
        "#### Example:\n",
        "Using the same sentence \"The cat sat on the mat\" and assuming \"sat\" is the target word with a context window size of 2, Skip-gram will try to predict the context words \"The,\" \"cat,\" \"on,\" and \"the\" from the target word \"sat.\"\n",
        "\n",
        "#### Architecture:\n",
        "The architecture involves the following steps:\n",
        "- Convert the target word to its corresponding word embedding.\n",
        "- Use this embedding to predict the context words through a neural network.\n",
        "\n",
        "### Explanation of Parameters:\n",
        "- Other parameters are already explained above for CBOW\n",
        "- `sg`: Specifies the Skip-gram architecture. (Setting sg=0 would use CBOW instead)"
      ],
      "metadata": {
        "id": "EtRryo9O_1m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip-gram model\n",
        "skipgram_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=2, sg=1, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "lv8vc9CKq3mL"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector and most similar words for a specific word\n",
        "target_word='machine'\n",
        "vector, similar_words = find_similar_words(skipgram_model, target_word)\n",
        "\n",
        "print(\"Vector for 'machine':\", vector)\n",
        "print(\"Words most similar to 'machine':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuJUW8MgA23h",
        "outputId": "fdeb2b9a-09fb-4411-8cb4-c09de90a8b31"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine': [ 8.7139240e-05  3.0821874e-03 -6.8086009e-03 -1.3650591e-03\n",
            "  7.6718316e-03  7.3448555e-03 -3.6722927e-03  2.6453969e-03\n",
            " -8.3143059e-03  6.2077623e-03 -4.6357708e-03 -3.1756633e-03\n",
            "  9.3123931e-03  8.7520888e-04  7.4898605e-03 -6.0778940e-03\n",
            "  5.1586907e-03  9.9251298e-03 -8.4566930e-03 -5.1376238e-03\n",
            " -7.0603210e-03 -4.8608501e-03 -3.7769042e-03 -8.5377069e-03\n",
            "  7.9546170e-03 -4.8412192e-03  8.4215924e-03  5.2663768e-03\n",
            " -6.5552192e-03  3.9525498e-03  5.4742219e-03 -7.4337036e-03\n",
            " -7.4008247e-03 -2.4778158e-03 -8.6272350e-03 -1.5785688e-03\n",
            " -3.9278960e-04  3.3041148e-03  1.4385493e-03 -8.7304221e-04\n",
            " -5.5887694e-03  1.7217635e-03 -9.0340531e-04  6.8027633e-03\n",
            "  3.9759362e-03  4.5307246e-03  1.4332269e-03 -2.7039147e-03\n",
            " -4.3643410e-03 -1.0367834e-03  1.4403542e-03 -2.6488120e-03\n",
            " -7.0668710e-03 -7.8039132e-03 -9.1294488e-03 -5.9293797e-03\n",
            " -1.8407891e-03 -4.3247347e-03 -6.4607803e-03 -3.7125184e-03\n",
            "  4.2900951e-03 -3.7411901e-03  8.3826398e-03  1.5306490e-03\n",
            " -7.2425427e-03  9.4342185e-03  7.6294700e-03  5.4991562e-03\n",
            " -6.8457206e-03  5.8259573e-03  4.0025120e-03  5.1837177e-03\n",
            "  4.2606066e-03  1.9424780e-03 -3.1638283e-03  8.3570601e-03\n",
            "  9.6084410e-03  3.7946589e-03 -2.8350796e-03  2.1456397e-06\n",
            "  1.2080364e-03 -8.4632207e-03 -8.2246587e-03 -2.3466600e-04\n",
            "  1.2290154e-03 -5.7420647e-03 -4.7249761e-03 -7.3551764e-03\n",
            "  8.3316443e-03  1.2355545e-04 -4.5103966e-03  5.7047061e-03\n",
            "  9.1811810e-03 -4.1035782e-03  7.9701729e-03  5.3761527e-03\n",
            "  5.8808988e-03  5.1821611e-04  8.2069123e-03 -7.0187366e-03]\n",
            "Words most similar to 'machine': [('Natural', 0.19925296306610107), ('Word2Vec', 0.17288964986801147), ('intelligence', 0.17111153900623322), ('language', 0.17042487859725952), ('Deep', 0.15282553434371948)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Reference:\n",
        "- [Paper on CBOW & Skip-Gram](https://arxiv.org/pdf/1301.3781)\n",
        "- [Explanation Video on CBOW & Skip-Gram](https://www.youtube.com/watch?v=QYrhJUBWJwA)"
      ],
      "metadata": {
        "id": "n3kZjO4vFfB8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe Word Embedding\n",
        "GloVe stands for Global Vectors for word representation and was developed by researchers at Stanford University. It is unsupervised learning algorithm aiming to generate word embeddings by aggregating global word co-occurrence matrices from a given corpus. To start with GloVe, first we have to download the pre-trained model hosted [here](https://nlp.stanford.edu/projects/glove/). A total of four pre-trained models are available there. Get your own choice.\n",
        "\n",
        "The basic idea behind the GloVe word embedding is to derive the relationship between the words from statistics.\n",
        "\n",
        "To use glove word embedding with our way, you first need to install python scipy and numpy libraries (if not installed already). Copy the below command to do so.\n",
        "\n",
        "```\n",
        "pip3 install scipy\n",
        "pip3 install numpy\n",
        "```"
      ],
      "metadata": {
        "id": "9bVTUsOIFiJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d watts2/glove6b50dtxt\n",
        "!unzip -q \"glove6b50dtxt.zip\" -d \"/content/data/\""
      ],
      "metadata": {
        "id": "bduYUi72F4a5",
        "outputId": "7ad444bb-3c3e-48f3-ce80-4946d549b05a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/watts2/glove6b50dtxt\n",
            "License(s): CC0-1.0\n",
            "Downloading glove6b50dtxt.zip to /content\n",
            " 99% 67.0M/67.7M [00:00<00:00, 140MB/s]\n",
            "100% 67.7M/67.7M [00:00<00:00, 131MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_filepath = 'data/glove.6B.50d.txt'\n",
        "embeddings_dict = {}\n",
        "\n",
        "with open(glove_filepath, 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = np.asarray(values[1:], \"float32\")\n",
        "        embeddings_dict[word] = vector\n",
        "\n",
        "# Function to find similar words based on cosine similarity\n",
        "def similar_words(word, embeddings_dict, top_n=10):\n",
        "    if word not in embeddings_dict:\n",
        "        return f\"Word '{word}' not in vocabulary\"\n",
        "\n",
        "    word_vector = embeddings_dict[word]\n",
        "    similarities = {}\n",
        "\n",
        "    for other_word, other_vector in embeddings_dict.items():\n",
        "        if other_word != word:\n",
        "            similarity = 1 - spatial.distance.cosine(word_vector, other_vector)\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "    # Sort words by similarity\n",
        "    sorted_words = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
        "    return sorted_words[:top_n]"
      ],
      "metadata": {
        "id": "61VttCTPA3zm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "word_to_check = \"machine\"\n",
        "top_similar_words = similar_words(word_to_check, embeddings_dict, top_n=6)\n",
        "\n",
        "print(f\"Words most similar to '{word_to_check}':\")\n",
        "for i, (word, similarity) in enumerate(top_similar_words):\n",
        "    print(f\"{i+1}. {word}: {similarity:.4f}\")"
      ],
      "metadata": {
        "id": "CbkuGbHcG6u0",
        "outputId": "45c14b2d-aaa6-4286-f60a-f67aa63f2722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'machine':\n",
            "1. machines: 0.8239\n",
            "2. device: 0.8176\n",
            "3. using: 0.7790\n",
            "4. gun: 0.7509\n",
            "5. used: 0.7493\n",
            "6. devices: 0.7369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText Word Embedding\n",
        "\n",
        "The FastText library, developed by the Facebook AI Research lab, is an open-source word-embedding library designed to provide more accurate and scalable solutions for processing large text data. FastText extends the principles of `Word2Vec` but introduces enhancements that allow it to capture the meanings of words more effectively, especially in morphologically rich languages.\n",
        "\n",
        "### How FastText Works\n",
        "\n",
        "Unlike Word2Vec, which treats each word as an atomic entity, FastText breaks down each word into a series of character n-grams and uses these n-grams to generate the word's embedding. For instance, for the word \"fasttext,\" the tri-grams (3-character n-grams) are:\n",
        "\n",
        "```\n",
        "<fa, fas, ast, stt, tte, tex, ext, xt>\n",
        "```\n",
        "\n",
        "FastText trains embedding vectors for each n-gram. The final embedding for a word is obtained by summing the embeddings of its n-grams. This approach allows FastText to represent words more flexibly and accurately, particularly for rare words and out-of-vocabulary (OOV) words.\n",
        "\n",
        "### Advantages of FastText Over Word2Vec\n",
        "\n",
        "1. **Handling Compound Words**:\n",
        "   - FastText can effectively represent compound words even if they are not present in the training data. For example, the word \"fasttext\" can be represented by combining the embeddings of its n-grams, which might appear in other words like \"fast\" and \"text.\"\n",
        "\n",
        "2. **Morphological Awareness**:\n",
        "   - FastText shares parameters among words with common roots. For example, \"fast,\" \"faster,\" and \"fastest\" share common n-grams, allowing FastText to utilize the morphological structure of words more efficiently. This results in better representations for words with shared prefixes or suffixes.\n",
        "\n",
        "### Implementation with Gensim\n",
        "\n",
        "Python provides the `gensim` library, which simplifies working with FastText. For preprocessing, we will use the `nltk` library. Below is a step-by-step guide on how to implement FastText using `gensim`.\n",
        "\n",
        "  ```sh\n",
        "  pip3 install nltk\n",
        "  pip3 install gensim\n",
        "  ```\n"
      ],
      "metadata": {
        "id": "Jm4JgrjPM8Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Train the FastText model\n",
        "model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1, workers=4)\n",
        "\n",
        "# Get the vector and most similar words for a specific word\n",
        "target_word='machine'\n",
        "vector, similar_words = find_similar_words(model, target_word)\n",
        "\n",
        "print(\"Vector for 'machine':\", vector)\n",
        "print(\"Words most similar to 'machine':\", similar_words)"
      ],
      "metadata": {
        "id": "Ld82Ys1ELpbO",
        "outputId": "d2c0dcd1-10fa-4539-fda3-a1ca95a12832",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine': [ 2.83578665e-05 -3.02281347e-04  1.28363830e-03 -4.09543602e-04\n",
            " -9.58424062e-04 -2.29730271e-03 -1.23520859e-03 -7.47971178e-04\n",
            " -6.38194091e-04 -3.37117526e-04  1.34325260e-03 -2.80743028e-04\n",
            "  4.69989376e-04  1.17863587e-03  7.65687088e-04 -7.21512304e-04\n",
            " -1.16958574e-03  1.38633850e-03  1.07712997e-03  6.79898832e-04\n",
            " -1.42542075e-03  3.80009500e-04 -9.12218296e-04 -5.88002367e-05\n",
            " -9.24882828e-04 -1.40186225e-04  2.67761829e-03 -7.75957655e-04\n",
            " -1.06496830e-03  1.75724761e-03  5.19282534e-04  3.10164236e-04\n",
            "  4.07238666e-04  3.79483186e-04 -4.56103327e-04  1.69317762e-03\n",
            " -3.87150969e-04  1.52309705e-03  4.18928539e-05 -1.80306830e-04\n",
            "  7.10254128e-04  2.60576315e-04  2.89674033e-04 -1.61156931e-03\n",
            " -1.56921073e-04 -1.13051408e-03 -8.83841480e-04  1.40559030e-04\n",
            " -1.63029635e-03 -1.18701919e-05  6.69965346e-04  2.76756613e-03\n",
            "  1.19824114e-03 -2.95652280e-04 -2.55553075e-03 -1.39934779e-03\n",
            "  3.53162264e-04 -1.04577083e-03 -8.54474434e-04 -3.42717103e-04\n",
            "  8.19901587e-04 -3.96175223e-04 -1.63880788e-04 -3.16180376e-04\n",
            "  4.61588170e-05  6.95065770e-04  1.44325313e-03  9.26179404e-04\n",
            "  8.81140237e-04  4.59887844e-04 -1.14281918e-03 -6.90040397e-05\n",
            " -5.65352151e-04 -1.27051200e-03 -2.21232476e-04  2.45066523e-03\n",
            " -2.71790777e-04  1.91381946e-03  4.08429565e-04 -2.35878921e-04\n",
            " -7.56687019e-04  5.94065641e-04 -5.97990351e-04 -5.33492188e-04\n",
            " -1.16748226e-04 -2.03223643e-03 -2.36273534e-03 -5.91916651e-05\n",
            " -5.24686882e-04 -3.90377128e-04  4.40638192e-04  4.34092799e-04\n",
            "  4.06448438e-04 -1.01936480e-03 -9.43678722e-04 -7.07358762e-04\n",
            " -4.28243249e-04  2.15825770e-04 -1.45804684e-03  1.30155345e-03]\n",
            "Words most similar to 'machine': [('processing', 0.19849617779254913), ('language', 0.1736741065979004), ('are', 0.15312983095645905), ('fields', 0.11571068316698074), ('capture', 0.11387857794761658)]\n"
          ]
        }
      ]
    }
  ]
}