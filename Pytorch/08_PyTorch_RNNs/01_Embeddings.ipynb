{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n",
        "\n",
        "## Embeddings in Natural Language Processing (NLP)\n",
        "\n",
        "In the context of natural language processing (NLP), \"embeddings\" refer to dense vector representations of words (or sometimes phrases and sentences) in a continuous vector space. These vector representations are learned through unsupervised machine learning techniques like Word2Vec, GloVe, or FastText, where words with similar meanings or appearing in similar contexts are mapped to vectors that are close together in the vector space.\n",
        "\n",
        "![Embeddings](https://miro.medium.com/max/600/1*UCKRYEj85S3eH1uv1vFfCw.gif)\n",
        "\n",
        "## Limitations of Traditional Word Representations\n",
        "\n",
        "### One-Hot Encoding\n",
        "Traditionally, words have been represented using one-hot encoding, where each word is represented as a sparse binary vector. In this representation, there is a 1 in the position corresponding to the word's index in the vocabulary and 0s everywhere else. However, one-hot encoded vectors have several limitations:\n",
        "\n",
        "![One hot encoding](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
        "\n",
        "- **High Dimensionality:** One-hot encoded vectors are very high-dimensional, with the dimensionality equal to the size of the vocabulary. This leads to increased computational complexity and storage requirements.\n",
        "- **Lack of Semantic Information:** One-hot vectors do not capture any semantic relationships between words. Each word is treated as an isolated entity with no notion of similarity or relatedness to other words.\n",
        "\n",
        "## Advantages of Word Embeddings\n",
        "\n",
        "Embeddings address the limitations of one-hot encoding and offer several advantages in NLP:\n",
        "\n",
        "### 1. Low-Dimensional Dense Representations\n",
        "Word embeddings are low-dimensional dense vectors, typically ranging from 50 to 300 dimensions. This makes them computationally efficient and memory-friendly compared to one-hot vectors.\n",
        "\n",
        "### 2. Semantic Relationships\n",
        "Embeddings capture semantic relationships between words. Words with similar meanings or appearing in similar contexts will have similar vector representations, enabling models to understand the meaning and context of words.\n",
        "\n",
        "### 3. Generalization\n",
        "Word embeddings allow NLP models to generalize better across different tasks and datasets. Pre-trained word embeddings can be used as features for various downstream tasks, even if the training data for the downstream task is limited.\n",
        "\n",
        "### 4. Out-of-Vocabulary (OOV) Words\n",
        "Word embeddings provide representations for words not seen during training (OOV words) by generalizing from the context of other words.\n",
        "\n",
        "### 5. Efficiency\n",
        "Once trained, word embeddings can be efficiently stored and reused, which is especially important for large-scale NLP applications.\n",
        "\n",
        "### 6. Capturing Analogies\n",
        "Word embeddings can capture analogical relationships like \"king\" - \"man\" + \"woman\" â‰ˆ \"queen,\" allowing models to perform analogy-based reasoning.\n",
        "\n",
        "In summary, embeddings are a powerful tool in NLP, offering a more efficient, semantically rich, and generalizable way to represent words compared to traditional methods like one-hot encoding."
      ],
      "metadata": {
        "id": "x6IdOlDbm0Xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec is a popular technique for learning word embeddings, which are dense vector representations of words in a continuous vector space. These embeddings capture semantic relationships between words, allowing machines to understand and work with words in a more meaningful way. Word2Vec was introduced by researchers at Google in 2013 and has since become one of the foundational techniques in natural language processing (NLP) and related fields.\n",
        "\n",
        "The basic idea behind Word2Vec is to represent each word in a high-dimensional vector space, where words with similar meanings or contexts are located close to each other. This is based on the distributional hypothesis, which posits that words appearing in similar contexts tend to have similar meanings. For example, in the sentences \"I love cats\" and \"I adore felines,\" the words \"love\" and \"adore\" are likely to be used in similar contexts and have similar semantic meanings.\n",
        "\n",
        "Word2Vec can be trained using two main architectures: Continuous Bag of Words (CBOW) and Skip-gram. Let's explore each of these in detail:\n",
        "\n",
        "<img src=\"../../assets/cbow_skipgram.png\">\n",
        "\n",
        "### 1. Continuous Bag of Words (CBOW)\n",
        "\n",
        "CBOW aims to predict a target word based on its surrounding context words. Given a sequence of words in a sentence, CBOW tries to predict the middle word based on the surrounding context words. The context window size determines how many words before and after the target word are considered as the context.\n",
        "\n",
        "#### Example:\n",
        "Consider the sentence: \"The cat sat on the mat.\" If we set the context window size to 2 and assume \"sat\" is the target word, CBOW will use the context words \"The,\" \"cat,\" \"on,\" and \"the\" to predict the word \"sat.\"\n",
        "\n",
        "#### Architecture:\n",
        "The architecture involves the following steps:\n",
        "- Convert the context words to their corresponding word embeddings.\n",
        "- Average these embeddings to create a context vector.\n",
        "- Use this context vector as input to a neural network to predict the target word.\n",
        "\n",
        "## Implementing Word2Vec in Python\n",
        "\n",
        "Python provides a package named `gensim` to make implementing Word2Vec straightforward. Here's how you can get started with it:\n",
        "\n",
        "### Installation\n",
        "First, install the `gensim` package if you haven't already:\n",
        "\n",
        "```bash\n",
        "pip install gensim\n",
        "```\n",
        "\n",
        "### Explanation of Parameters:\n",
        "- `sentences`: The input data, which is a list of tokenized sentences.\n",
        "- `vector_size`: The dimensionality of the word vectors.\n",
        "- `window`: The maximum distance between the current and predicted word within a sentence.\n",
        "- `min_count`: Ignores all words with total frequency lower than this.\n",
        "- `workers`: The number of worker threads to train the model.\n",
        "\n",
        "Word2Vec with `gensim` is a powerful tool that simplifies the creation of word embeddings, making it easier to integrate semantic understanding into your NLP applications."
      ],
      "metadata": {
        "id": "8oF6Q2AConJm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUYW6vf2mpCe",
        "outputId": "3d46eb49-7e09-425c-a1f7-9e4798ab3ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download the NLTK tokenizer data if not already done\n",
        "nltk.download('punkt')\n",
        "\n",
        "# List of sentences for training (unsplit)\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Natural language processing is exciting\",\n",
        "    \"Word2Vec creates word embeddings\",\n",
        "    \"Gensim is a useful library\",\n",
        "    \"Deep learning is a subset of machine learning\",\n",
        "    \"Embeddings capture semantic relationships\",\n",
        "    \"Python is a popular programming language\",\n",
        "    \"Artificial intelligence and machine learning are related fields\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(tokenized_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cK0BXnrWotml",
        "outputId": "0b628bed-fd74-4481-9ce3-ba3c2151d6dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'machine', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model using the tokenized sentences\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=2, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "R2bVHh_MpbVC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find similar words using the trained model\n",
        "def find_similar_words(model, word, top_n=5):\n",
        "    vector = model.wv[word]\n",
        "    similar_words = model.wv.most_similar(positive=[word], topn=top_n)\n",
        "    return vector, similar_words"
      ],
      "metadata": {
        "id": "W_58w5tLqElP"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector and most similar words for a specific word\n",
        "target_word='machine'\n",
        "vector, similar_words = find_similar_words(model, target_word)\n",
        "\n",
        "print(\"Vector for 'machine':\", vector)\n",
        "print(\"Words most similar to 'machine':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-wPDalqqbpv",
        "outputId": "45224db0-867b-4c60-b7d6-da389a9467ce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine': [ 9.0809692e-05  3.0832055e-03 -6.8151313e-03 -1.3689728e-03\n",
            "  7.6685268e-03  7.3423618e-03 -3.6741595e-03  2.6473312e-03\n",
            " -8.3173197e-03  6.2057734e-03 -4.6351124e-03 -3.1670930e-03\n",
            "  9.3112951e-03  8.7239651e-04  7.4903150e-03 -6.0771578e-03\n",
            "  5.1645460e-03  9.9195987e-03 -8.4572462e-03 -5.1375022e-03\n",
            " -7.0665088e-03 -4.8636729e-03 -3.7799729e-03 -8.5374974e-03\n",
            "  7.9519451e-03 -4.8466586e-03  8.4186336e-03  5.2713170e-03\n",
            " -6.5517426e-03  3.9549218e-03  5.4736012e-03 -7.4305790e-03\n",
            " -7.4054408e-03 -2.4740247e-03 -8.6299535e-03 -1.5781232e-03\n",
            " -3.9694359e-04  3.3004046e-03  1.4376161e-03 -8.7451038e-04\n",
            " -5.5918437e-03  1.7300018e-03 -8.9923030e-04  6.7969901e-03\n",
            "  3.9745839e-03  4.5312811e-03  1.4351372e-03 -2.7016769e-03\n",
            " -4.3661897e-03 -1.0324767e-03  1.4385569e-03 -2.6458562e-03\n",
            " -7.0720618e-03 -7.8036557e-03 -9.1262041e-03 -5.9363050e-03\n",
            " -1.8445110e-03 -4.3226061e-03 -6.4571970e-03 -3.7157002e-03\n",
            "  4.2899637e-03 -3.7400872e-03  8.3837649e-03  1.5315602e-03\n",
            " -7.2425385e-03  9.4318893e-03  7.6317666e-03  5.4961131e-03\n",
            " -6.8513905e-03  5.8209687e-03  4.0058908e-03  5.1868521e-03\n",
            "  4.2576790e-03  1.9383364e-03 -3.1670255e-03  8.3557712e-03\n",
            "  9.6082436e-03  3.7972576e-03 -2.8360703e-03  3.4182870e-06\n",
            "  1.2130676e-03 -8.4575703e-03 -8.2244556e-03 -2.2725610e-04\n",
            "  1.2358051e-03 -5.7472461e-03 -4.7265878e-03 -7.3482059e-03\n",
            "  8.3350940e-03  1.2313284e-04 -4.5144265e-03  5.7042022e-03\n",
            "  9.1793118e-03 -4.1021546e-03  7.9718847e-03  5.3714202e-03\n",
            "  5.8786725e-03  5.1741279e-04  8.2144625e-03 -7.0180092e-03]\n",
            "Words most similar to 'machine': [('Natural', 0.1991470903158188), ('Word2Vec', 0.17297668755054474), ('intelligence', 0.17133425176143646), ('language', 0.17026880383491516), ('Deep', 0.152846097946167)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Skip-gram\n",
        "\n",
        "Skip-gram, on the other hand, works in the opposite manner to CBOW. It aims to predict the context words given a target word. Skip-gram is particularly useful for smaller datasets and when you want to capture more information about infrequent words.\n",
        "\n",
        "#### Example:\n",
        "Using the same sentence \"The cat sat on the mat\" and assuming \"sat\" is the target word with a context window size of 2, Skip-gram will try to predict the context words \"The,\" \"cat,\" \"on,\" and \"the\" from the target word \"sat.\"\n",
        "\n",
        "#### Architecture:\n",
        "The architecture involves the following steps:\n",
        "- Convert the target word to its corresponding word embedding.\n",
        "- Use this embedding to predict the context words through a neural network.\n",
        "\n",
        "### Explanation of Parameters:\n",
        "- Other parameters are already explained above for CBOW\n",
        "- `sg`: Specifies the Skip-gram architecture. (Setting sg=0 would use CBOW instead)"
      ],
      "metadata": {
        "id": "EtRryo9O_1m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip-gram model\n",
        "skipgram_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=2, sg=1, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "lv8vc9CKq3mL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector and most similar words for a specific word\n",
        "target_word='machine'\n",
        "vector, similar_words = find_similar_words(skipgram_model, target_word)\n",
        "\n",
        "print(\"Vector for 'machine':\", vector)\n",
        "print(\"Words most similar to 'machine':\", similar_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuJUW8MgA23h",
        "outputId": "c9d0c7f9-15e2-49ab-8101-f2697465a364"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'machine': [ 8.7139240e-05  3.0821874e-03 -6.8086009e-03 -1.3650591e-03\n",
            "  7.6718316e-03  7.3448555e-03 -3.6722927e-03  2.6453969e-03\n",
            " -8.3143059e-03  6.2077623e-03 -4.6357708e-03 -3.1756633e-03\n",
            "  9.3123931e-03  8.7520888e-04  7.4898605e-03 -6.0778940e-03\n",
            "  5.1586907e-03  9.9251298e-03 -8.4566930e-03 -5.1376238e-03\n",
            " -7.0603210e-03 -4.8608501e-03 -3.7769042e-03 -8.5377069e-03\n",
            "  7.9546170e-03 -4.8412192e-03  8.4215924e-03  5.2663768e-03\n",
            " -6.5552192e-03  3.9525498e-03  5.4742219e-03 -7.4337036e-03\n",
            " -7.4008247e-03 -2.4778158e-03 -8.6272350e-03 -1.5785688e-03\n",
            " -3.9278960e-04  3.3041148e-03  1.4385493e-03 -8.7304221e-04\n",
            " -5.5887694e-03  1.7217635e-03 -9.0340531e-04  6.8027633e-03\n",
            "  3.9759362e-03  4.5307246e-03  1.4332269e-03 -2.7039147e-03\n",
            " -4.3643410e-03 -1.0367834e-03  1.4403542e-03 -2.6488120e-03\n",
            " -7.0668710e-03 -7.8039132e-03 -9.1294488e-03 -5.9293797e-03\n",
            " -1.8407891e-03 -4.3247347e-03 -6.4607803e-03 -3.7125184e-03\n",
            "  4.2900951e-03 -3.7411901e-03  8.3826398e-03  1.5306490e-03\n",
            " -7.2425427e-03  9.4342185e-03  7.6294700e-03  5.4991562e-03\n",
            " -6.8457206e-03  5.8259573e-03  4.0025120e-03  5.1837177e-03\n",
            "  4.2606066e-03  1.9424780e-03 -3.1638283e-03  8.3570601e-03\n",
            "  9.6084410e-03  3.7946589e-03 -2.8350796e-03  2.1456397e-06\n",
            "  1.2080364e-03 -8.4632207e-03 -8.2246587e-03 -2.3466600e-04\n",
            "  1.2290154e-03 -5.7420647e-03 -4.7249761e-03 -7.3551764e-03\n",
            "  8.3316443e-03  1.2355545e-04 -4.5103966e-03  5.7047061e-03\n",
            "  9.1811810e-03 -4.1035782e-03  7.9701729e-03  5.3761527e-03\n",
            "  5.8808988e-03  5.1821611e-04  8.2069123e-03 -7.0187366e-03]\n",
            "Words most similar to 'machine': [('Natural', 0.19925296306610107), ('Word2Vec', 0.17288964986801147), ('intelligence', 0.17111153900623322), ('language', 0.17042487859725952), ('Deep', 0.15282553434371948)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "61VttCTPA3zm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}