{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm\n",
    "\n",
    "Naive Bayes is a simple yet powerful classification algorithm based on Bayes' theorem. It is widely used in various machine learning applications, such as text classification, spam filtering, sentiment analysis, and recommendation systems.\n",
    "\n",
    "**Assumption:** Each feature is conditionally independent of every other feature.\n",
    "\n",
    "## Inspiration behind Naive Bayes\n",
    "\n",
    "Naive Bayes is inspired by Bayes' theorem, which is a fundamental concept in probability theory. Bayes' theorem allows us to update our beliefs about an event based on new evidence. It calculates the probability of an event given prior knowledge or evidence.\n",
    "\n",
    "Bayes' theorem is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A) * P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "where:\n",
    "\n",
    "- $P(A|B)$ is the probability of event $A$ occurring given that event $B$ has occurred.\n",
    "- $P(B|A)$ is the probability of event $B$ occurring given that event $A$ has occurred.\n",
    "- $P(A)$ and $P(B)$ are the probabilities of events $A$ and $B$ occurring independently.\n",
    "\n",
    "To derive the Naive Bayes equation using Bayes' theorem, let's consider a classification problem with a set of features or predictors $X = {X1, X2, ..., Xn}$ and a target class $C$. We want to calculate the probability of the class $C$ given the features $X$, denoted as $P(C|X)$.\n",
    "\n",
    "\\begin{align*}\n",
    "P(C|X) &= \\frac{P(X|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1, X_2, \\ldots, X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(X_1|C) \\cdot P(X_2|C) \\cdot \\ldots \\cdot P(X_n|C) \\cdot P(C)}{P(X)} \\\\\n",
    "&= \\frac{P(C) \\cdot \\prod P(X_i|C)}{P(X)}\n",
    "\\end{align*}\n",
    "\n",
    "The final class label of $X$ is the class that maximizes the value of $P(C|X)$.\n",
    "\n",
    "## Naive Bayes Best and Worst Case Senarios\n",
    "\n",
    "| **Best Case**                                                                                       | **Worst Case**                                                                                       |\n",
    "|-----------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n",
    "| - Feature independence assumption holds or is nearly true.                                          | - Features are highly correlated, violating the independence assumption.                             |\n",
    "| - Excellent for text classification tasks.                                                          | - Suboptimal performance when feature independence is not realistic.                                  |\n",
    "| - Efficient with high-dimensional data.                                                             | - Insufficient training data.                                                                         |\n",
    "| - Performs well even with limited training data.                                                    | - Other algorithms like logistic regression or decision trees may be more appropriate in such cases.  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Additive Smoothing\n",
    "\n",
    "In case of Naive Bayes, we mostly deal with textual data. So in some scenarios, we may arise with the condition where we encouter a word $w_o$ that is not present in our training data.\n",
    "\n",
    "$P(C|X) = \\frac{P(C) \\cdot P(w_1|C) \\cdot P(w_2|C) \\cdot \\ldots \\cdot P(w_o|C)}{P(w)}$\n",
    "\n",
    "- If we drop this word $w_o$ then from above equation we get $P(w_o|C) = 1$, which cannot be true.\n",
    "- If we calculate its probability as usual, $P(w_o|C) = 0$, which makes the entire equation $0$.\n",
    "\n",
    "To overcome this issue, Laplace Additive Smooting comes into play, which suggest the following equation:\n",
    "\n",
    "$P(w_o|C) = \\frac{n + \\alpha}{N + \\alpha \\cdot k}$\n",
    "\n",
    "where,\n",
    "\n",
    "- n = Number of training data containing word $w_o$\n",
    "- $\\alpha$ = Hyperparameter (typically $\\alpha$ = 1)\n",
    "- N = Total number of training data\n",
    "- k = Number of distinct values $w_o$ can take\n",
    "\n",
    "## Log Probabilities for Numerical Stability\n",
    "\n",
    "Probability lies between 0 and 1. When we multiply large number of probabilities numerical unstability occurs, i.e.\n",
    "\n",
    "$P = P1*P2*P3*P4 = 0.2*0.1*0.1*0.2 = 0.0004$\n",
    "\n",
    "In python double precision has only 16 significant values, which may overflow for a large sequence of words. So there is the possibility of numerical underflow and python starts rounding numbers which creates deviation in the result.\n",
    "\n",
    "We can use `log` to convert these numerical product to sum:\n",
    "\n",
    "$log(P) = log(P1*P2*P3*P4) = log(p1) + log(p2) + log(p3) + log(p4) = 0.2 + 0.1 +0.1 + 0.2 = 0.6$\n",
    "\n",
    "## Bias-Variance Tradeoff\n",
    "\n",
    "We know,\n",
    "\n",
    "- High bias -> Underfit\n",
    "- High variance -> Overfit\n",
    "\n",
    "In any algorithms hyperparameter play their role for bias-variance tradeoff. In case of Naive Bayes, $\\alpha$ (as discussed under Laplace Additive Smoothing) is the hyperparameter.\n",
    "\n",
    "Consider a balanced dataset with 1000 positive (y=1) and 1000 negative (y=0) data points. Then the probability of any word $w_i$, (let's say $w_i$ occured twice in the positive training data) to belong to positive class is given by:\n",
    "\n",
    "$P(w_i|y=1) = \\frac{n + \\alpha}{N + \\alpha \\cdot k} = \\frac{n + \\alpha}{N + \\alpha \\cdot k}$\n",
    "\n",
    "**Case I:** When $\\alpha$ is small (let's say $\\alpha$=0)\n",
    "\n",
    "$P(w_i|y=1) = \\frac{2 + 0}{1000 + 0 \\cdot 2} = \\frac{2}{1000} = 0.002$\n",
    "\n",
    "Here we took $k=2$ assuming $k$ can take two values `present` or `not present`.\n",
    "\n",
    "So when we remove $w_i$ from the training data, i.e. $n=0$, we are making a small change by removing 2 from the numerator. However this small change makes the overall  probability zero, which is a drastic change in probability. In another words a small change in training data created a huge change in output, i.e. variance is high. So, from this we can draw an insight that when $\\alpha$ is small, we are possibly overfitting.\n",
    "\n",
    "**Case II:** When $\\alpha$ is large (let's say $\\alpha$=100)\n",
    "\n",
    "$P(w_i|y=1) = \\frac{2 + 1000}{1000 + 1000 \\cdot 2} = \\frac{1002}{3000} = 0.334$\n",
    "\n",
    "When we remove $w_i$ from the training data, i.e. $n=0$, the probability becomes:\n",
    "\n",
    "$P(w_i|y=1) = \\frac{0 + 1000}{1000 + 1000 \\cdot 2} = \\frac{1000}{3000} = 0.333$\n",
    "\n",
    "This looks like removing $w_i$ from the training data creates almost no change in the output. Also the result of Naive Bayes classifier for both classes looks like:\n",
    "\n",
    "$P(y=1|w_1, w_2, \\ldots, w_n) = \\frac{P(y=1) \\cdot \\prod P(w_i|y=1)}{P(w)} ≈ \\frac{1}{2} \\cdot \\frac{P(y=1)}{P(w)}$\n",
    "\n",
    "$P(y=0|w_1, w_2, \\ldots, w_n) = \\frac{P(y=0) \\cdot \\prod P(w_i|y=0)}{P(w)} ≈ \\frac{1}{2} \\cdot \\frac{P(y=1)}{P(w)}$\n",
    "\n",
    "We see, in both the cases $\\prod P(w_i|y=...)$ becomes nearly equals to $\\frac{1}{2}$, no matter what the value of $w_i$ be. So our model does not have much to do i.e. we are underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Naïve Bayes Model:\n",
    "\n",
    "There are three types of Naive Bayes Model, which are given below:\n",
    "\n",
    "- **Gaussian:** The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n",
    "    \n",
    "- **Multinomial:** The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc. The classifier uses the frequency of words for the predictors.\n",
    "    \n",
    "- **Bernoulli:** The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Naive Bayes Classifier\n",
    "\n",
    "- **Spam Email Filtering:** Classifies emails as spam or non-spam based on features.\n",
    "- **Text Classification:** Used in sentiment analysis, document categorization, and topic classification.\n",
    "- **Medical Diagnosis:** Helps in predicting the likelihood of a disease based on symptoms.\n",
    "- **Credit Scoring:** Evaluates creditworthiness of individuals for loan approval.\n",
    "- **Weather Prediction:** Classifies weather conditions based on various factors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
